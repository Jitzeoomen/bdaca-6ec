% !TeX document-id = {f19fb972-db1f-447e-9d78-531139c30778}
% !BIB program = biber
\documentclass[handout]{beamer}
%\documentclass[compress]{beamer}
\usepackage[T1]{fontenc}
\usepackage{pifont}
\usetheme[block=fill,subsectionpage=progressbar,sectionpage=progressbar]{metropolis} 

\usepackage{wasysym}
\usepackage{etoolbox}
\usepackage[utf8]{inputenc}

\usepackage{threeparttable}
\usepackage{subcaption}

\usepackage{tikz-qtree}
\setbeamercovered{still covered={\opaqueness<1->{5}},again covered={\opaqueness<1->{100}}}


\usepackage{listings}

\lstset{
	basicstyle=\scriptsize\ttfamily,
	columns=flexible,
	breaklines=true,
	numbers=left,
	%stepsize=1,
	numberstyle=\tiny,
	backgroundcolor=\color[rgb]{0.85,0.90,1}
}



\lstnewenvironment{lstlistingoutput}{\lstset{basicstyle=\footnotesize\ttfamily,
		columns=flexible,
		breaklines=true,
		numbers=left,
		%stepsize=1,
		numberstyle=\tiny,
		backgroundcolor=\color[rgb]{.7,.7,.7}}}{}


\lstnewenvironment{lstlistingoutputtiny}{\lstset{basicstyle=\tiny\ttfamily,
		columns=flexible,
		breaklines=true,
		numbers=left,
		%stepsize=1,
		numberstyle=\tiny,
		backgroundcolor=\color[rgb]{.7,.7,.7}}}{}



\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[style=apa, backend = biber]{biblatex}
\DeclareLanguageMapping{american}{american-UoN}
\addbibresource{../../bdaca.bib}
\renewcommand*{\bibfont}{\tiny}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,matrix}
\usepackage{multicol}

\usepackage{subcaption}

\usepackage{booktabs}
\usepackage{graphicx}

\graphicspath{{../../pictures/}}

\makeatletter
\setbeamertemplate{headline}{%
	\begin{beamercolorbox}[colsep=1.5pt]{upper separation line head}
	\end{beamercolorbox}
	\begin{beamercolorbox}{section in head/foot}
		\vskip2pt\insertnavigation{\paperwidth}\vskip2pt
	\end{beamercolorbox}%
	\begin{beamercolorbox}[colsep=1.5pt]{lower separation line head}
	\end{beamercolorbox}
}
\makeatother



\setbeamercolor{section in head/foot}{fg=normal text.bg, bg=structure.fg}



\newcommand{\question}[1]{
	\begin{frame}[plain]
	\begin{columns}
		\column{.3\textwidth}
		\makebox[\columnwidth]{
			\includegraphics[width=\columnwidth,height=\paperheight,keepaspectratio]{mannetje.png}}
		\column{.7\textwidth}
		\large
		\textcolor{orange}{\textbf{\emph{#1}}}
	\end{columns}
\end{frame}}

\newcommand{\instruction}[1]{\emph{\textcolor{gray}{[#1]}}}


\title[Big Data and Automated Content Analysis]{\textbf{Big Data \& Automated Content Analysis} \\ Week 8 -- Monday: »Supervised Approaches to Text Analysis «}
\author[Anne Kroon]{Anne Kroon \\ \footnotesize{a.c.kroon@uva.nl \\@annekroon} \\ }
\date{17 May 2021}
\institute[UvA]{Afdeling Communicatiewetenschap \\Universiteit van Amsterdam}

\begin{document}
	

\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw]
\tikzstyle{pijltje} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
minimum height=2em, text width=4em, text centered,]

	
	\begin{frame}{}
		\titlepage
	\end{frame}
	
	\begin{frame}{Today}
		\tableofcontents
	\end{frame}
	

\section[Machine Learning]{Machine Learning}


\begin{frame}{A familiar picture by now}

\makebox[\linewidth]{
\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{boumanstrilling2016}}
\cite{Boumans2016}
\end{frame}



\begin{frame}{Some terminology }
\begin{columns}[t]
	\column{.5\textwidth}
	
	\begin{block}<1-4>{Supervised machine learning}
		You have a dataset with both predictor and outcome (independent and dependent variables; features and labels) --- a \emph{labeled} dataset.
		\onslide<2>{
			\footnotesize{Think of regression: You measured \texttt{x1}, \texttt{x2}, \texttt{x3} and you want to predict \texttt{y}, which you also measured}}
	\end{block}
	
	\column{.5\textwidth}
	
	\begin{block}<3->{Unsupervised machine learning}
		You have no labels. \onslide<4>{(\footnotesize{You did not measure \texttt{y})}}\\
		\onslide<5>{\textbf{Again, you already know some techniques to find out how \texttt{x1}, \texttt{x2},\ldots \texttt{x\_i} co-occur from other courses:} \begin{itemize}
				\item Principal Component Analysis (PCA) and Singular Value Decomposition (SVD)
				\item Cluster analysis
				\item Topic modelling
				\item \ldots
			\end{itemize}
		}
	\end{block}
	
\end{columns}

\end{frame}


\begin{frame}{Let's distinguish four use cases\ldots}

\begin{enumerate}
\item Finding similar variables (dimensionality reduction) -- unsupervised
\item Finding similar cases (clustering) -- unsupervised
\item Predicting a continous variable (regression) -- supervised
\item Predicting group membership (classification) -- supervised
\end{enumerate}
\end{frame}


\begin{frame}[plain]
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllll}
	& x1 & x2 & x3 & x4 & x5 & y \\
	case1 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & \ding{110} \\
	case2 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & \ding{110}\\
	case3 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & \ding{110}\\
	case4 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & \ding{110}\\
\end{tabular}%
}
\end{table}
\end{frame}



\begin{frame}[plain]
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllll}
& \textcolor{orange}{x1} & x2 & \textcolor{orange}{x3}& \textcolor{blue}{x4} & \textcolor{blue}{x5} & \textcolor{gray}{(y)} \\
case1 & \textcolor{orange}{\ding{110}}  & \ding{110}  & \textcolor{orange}{\ding{110}}  & \textcolor{blue}{\ding{110}} & \textcolor{blue}{\ding{110}} & \textcolor{gray}{(\ding{110})} \\
case2 & \textcolor{orange}{\ding{110}}  & \ding{110}  & \textcolor{orange}{\ding{110}}  & \textcolor{blue}{\ding{110}} & \textcolor{blue}{\ding{110}} & \textcolor{gray}{(\ding{110})} \\
case3 & \textcolor{orange}{\ding{110}}  & \ding{110}  & \textcolor{orange}{\ding{110}}  & \textcolor{blue}{\ding{110}} & \textcolor{blue}{\ding{110}} & \textcolor{gray}{(\ding{110})} \\
case4 & \textcolor{orange}{\ding{110}}  & \ding{110}  & \textcolor{orange}{\ding{110}}  & \textcolor{blue}{\ding{110}} & \textcolor{blue}{\ding{110}} & \textcolor{gray}{(\ding{110})} \\
\end{tabular}%
}
\end{table}
Dimensionality reduction: finding similar variables (features)
\end{frame}


\begin{frame}[plain]
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllll}
& x1 & x2 & x3 & x4 & x5 & \textcolor{gray}{(y)} \\
\textcolor{orange}{case1} & \textcolor{orange}{\ding{110}}  & \textcolor{orange}{\ding{110}}  &\textcolor{orange}{\ding{110}}  &\textcolor{orange}{\ding{110}}   & \textcolor{orange}{\ding{110}} & \textcolor{gray}{(\ding{110})} \\
\textcolor{blue}{case2} & \textcolor{blue}{\ding{110}}  & \textcolor{blue}{\ding{110}}  &\textcolor{blue}{\ding{110}}  &\textcolor{blue}{\ding{110}}   & \textcolor{blue}{\ding{110}} & \textcolor{gray}{(\ding{110})} \\
\textcolor{orange}{case3} & \textcolor{orange}{\ding{110}}  & \textcolor{orange}{\ding{110}}  &\textcolor{orange}{\ding{110}}  &\textcolor{orange}{\ding{110}}   & \textcolor{orange}{\ding{110}} & \textcolor{gray}{(\ding{110})} \\
\textcolor{orange}{case4} & \textcolor{orange}{\ding{110}}  & \textcolor{orange}{\ding{110}}  &\textcolor{orange}{\ding{110}}  &\textcolor{orange}{\ding{110}}   & \textcolor{orange}{\ding{110}} & \textcolor{gray}{(\ding{110})} \\
\end{tabular}%
}
\end{table}
Clustering: finding similar cases
\end{frame}



\begin{frame}[plain]
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllllll}
& x1 & x2 & x3 & x4 & x5 & $\rightarrow$ & \textcolor{orange}{y} \\
case1 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & $\rightarrow$ &\textcolor{orange}{\ding{110}}  \\
case2 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & $\rightarrow$ &\textcolor{orange}{\ding{110}} \\
case3 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & $\rightarrow$ &\textcolor{orange}{\ding{110}} \\
case4 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & $\rightarrow$ &\textcolor{orange}{\ding{110}} \\
&&&&&&& \\
new case & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & $\rightarrow$ &\textbf{\textcolor{orange}{?}} \\
\end{tabular}%
}
Regression and classification: learn how to predict $y$.
\end{table}
\end{frame}




\begin{frame}[plain]
\textbf{Note, again, that the \ding{110} signs can be \emph{anything}}.
For us, often word counts or $tf\cdot$ idf scores ($x$) and, for supervised approaches, a topic, a sentiment, or similar ($y$). 

But it could also be pixel colors or clicks on links or anything else.
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllll}
& x1 & x2 & x3 & x4 & x5 & y \\
case1 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & \ding{110} \\
case2 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & \ding{110}\\
case3 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & \ding{110}\\
case4 & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110}  & \ding{110} & \ding{110}\\
\end{tabular}%
}
\end{table}
\end{frame}


\section{Predicting things}




\subsection{You have done it before!}
\begin{frame}{You have done it before!}
\begin{block}{Regression}<2->
	\begin{enumerate}
		\item<3-> Based on your data, you estimate some regression equation 	$y_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \varepsilon_i$
		\item<4-> Even if you have some \emph{new unseen data}, you can estimate your expected outcome $\hat{y}$!
		\item<5-> Example: You estimated a regression equation where $y$ is newspaper reading in days/week: $y = -.8 + .4 \times man + .08 \times age$
		\item<6-> You could now calculate $\hat{y}$ for a man of 20 years and a woman of 40 years -- \emph{even if no such person exists in your dataset}: \\
		$\hat{y}_{man20} = -.8 + .4 \times 1 + .08 \times 20 = 1.2$ \\
		$\hat{y}_{woman40} = -.8 + .4 \times 0 + .08 \times 40 = 2.4$
	\end{enumerate}
\end{block}	

\end{frame}



\begin{frame}{}
\huge{This is\\ Supervised Machine Learning!}
\end{frame}

\begin{frame}{\ldots but\ldots}
\begin{itemize}
\item<1-> We will only use \emph{half} {\tiny{(or another fraction)}} of our data to estimate the model, so that we can use the other half to check if our predictions match the manual coding (``labeled data'',``annotated data'' in SML-lingo)
\begin{itemize}
\item<2->e.g., 2000 labeled cases, 1000 for training, 1000 for testing --- if successful, run on 100,000 unlabeled cases
\end{itemize}
\item<3-> We use many more independent variables (``features'')
\item<4-> Typically, IVs are word frequencies (often weighted, e.g. tf$\times$idf) ($\Rightarrow$BOW-representation)
\end{itemize}
\end{frame}


\subsection{From regression to classification}

\begin{frame}[standout]
In the machine learning world, predicting some continous value is referred to as a \textcolor{orange}{regression} task. If we want to predict a binary or categorical variable, we call it a \textcolor{orange}{classification} task.

(quite confusingly, even if we use a logistic regression for the latter)
\end{frame}


\begin{frame}{Classification tasks}
For many computational approaches, we are actually not that interested in predicting a continous value. Typical questions include:
\begin{itemize}
\item Is this article about topic A, B, C, D, or E?
\item Is this review positive or negative?
\item Does this text contain frame F?
\item Is this satire? 
\item Is this misinformation?
\item Given past behavior, can I predict the next click?
\end{itemize}
\end{frame}



\begin{frame}[plain]
\begin{columns}[]
\column{.5\textwidth}

{\tiny{http://commons.wikimedia.org/wiki/File:Precisionrecall.svg}}
\makebox[\linewidth]{
\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{../pictures/precisionrecall.png}}

\column{.5\textwidth}
\begin{block}{Some measures}
\begin{itemize}
\item Accuracy
\item Recall
\item Precision
\item $\text{F1}=2\cdot \frac{\text{precision}\cdot \text{recall}}{\text{precision}+\text{recall}}$
\item AUC (Area under curve) $[0,1]$, $0.5=$ random guessing
\end{itemize}
\end{block}


\end{columns}

\end{frame}





\begin{frame}{Different classification algorithms}

\begin{itemize}[<+->]
\item It is an empirical question which one works best
\item We typically try several ones and select the best
\item (remember: we have a test dataset that we did \emph{not} use to train the model, so that we can assess how well it predicts the test labels based on the test features)
\end{itemize}
(to make it easier, imagine a binary classfication ("positive"/"negative"), but it doesn't really matter whether there are two or more labels)
\end{frame}




%
%
%\begin{frame}{Naïve Bayes}
%\begin{block}{Bayes' theorem}
%$$ P(A \mid B) = \frac{P(B \mid A) \times P(A)}{P(B)} $$
%\end{block}
%\pause
%\textcolor{red}{A = Text is about sports\\
%B = Text contains `very', `close', `game'}.
%\pause
%Furthermore, we simply multiply the propabilities for the features:
%\textcolor{red}{$$P(B) = P(very\, close\, game) = P(very) \times P(close) \times P(game)$$}
%We can fill in all values by counting how many articles are about sports, and how often the words occur in these texts.
%\vspace{0.3cm}
%\footnotesize{
%(Fully elaborated example on \url{https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/})}
%\end{frame}
%
%\begin{frame}{Naïve Bayes}
%\begin{itemize}[<+->]
%\item It's ``naïve'' because the features are treated as completely independent ($\neq$ ``controlling'' in regression analysis)
%\item It's fast and easy
%\item It's a good \emph{baseline} for binary classification problems
%\end{itemize}
%\end{frame}
%
%
%
%
%\begin{frame}{Na\"ive Bayes}
%$$ P(\rm{label} \mid \rm{features}) =$$
%$$ \frac{P(x_1 \mid label) \cdot P(x_2 \mid \rm{label})\ \cdot P(x_3 \mid label) \cdot P(label)}{P(x_1) \cdot P(x_2) \cdot P(x_3)}$$.
%
%
%\begin{itemize}
%\item Formulas always look intimidating, but we only need to fill in how many documents containing feature $x_n$ have the label, how often the label occurs, and how often each feature occurs
%\item Also for computers, this is \emph{really easy and fast}
%\item Weird assumption: features are independent
%\item Often used as a baseline
%\end{itemize}
%\end{frame}
%
%
%
%
%\begin{frame}{Logistic Regression}
%\begin{block}{Probability of a binary outcome in a regression model}
%$$p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)}}$$
%\end{block}
%Just like in OLS regression, we have an intercept and regression coefficients. 
%We use a threshold (default: 0.5) and above, we assign the positive label (`good movie'), below, the negative label (`bad movie').
%\end{frame}
%\begin{frame}{Logistic Regression}
%\begin{itemize}[<+->]
%\item The features are \emph{not} independent.
%\item Computationally more expensive than Naïve Bayes
%\item We can get probabilities instead of just a label
%\item That allows us to say how sure we are for a specific case
%\item \ldots or to change the threshold to change our precision/recall-tradeoff
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Support Vector Machines}
%\begin{columns}
%\column{.5\linewidth}
%\begin{itemize}
%\item	Idea: Find a hyperplane that best seperates your cases
%\item Can be linear, but does not have to be (depends on the so-called kernel you choose)
%\item Very popular 
%\end{itemize}
%\column{.5\linewidth}	
%\includegraphics[width=.8\linewidth,height=.5\paperheight,keepaspectratio]{../pictures/svm}
%\tiny{\url{https://upload.wikimedia.org/wikipedia/commons/b/b5/Svm\_separating\_hyperplanes\_\%28SVG\%29.svg}}
%\end{columns}
%\vfill
%\footnotesize{(Further reading: \url{https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/)}}
%\end{frame}
%
%\begin{frame}{SVM vs logistic regression}
%\begin{itemize}
%\item for \emph{linearly separable} classes not much difference
%\item with the right hyperparameters, SVM is less sensitive to outliers
%\item biggest advantage: with the \emph{kernel trick}, data can be transformed that they \emph{become} linearily separable
%\end{itemize}
%\end{frame}
%
%
%\begin{frame}{Decision Trees and Random Forests}
%\begin{columns}
%\column{.5\linewidth}
%\begin{itemize}[<+->]
%\item Model problem as a series of decisions (e.g., if cloudy then \ldots if temperature > 30 degrees then \ldots)
%\item Order and cutoff-points are determined by an algorithm
%\item Big advantage: Model non-linear relationships
%\item And: They are easy to interpret (!) (``white box'')
%\end{itemize}
%\column{.5\linewidth}	
%\includegraphics[width=.8\linewidth,height=.5\paperheight,keepaspectratio]{../pictures/decisiontree}
%\tiny{\url{https://upload.wikimedia.org/wikipedia/en/4/4f/GEP\_decision\_tree\_with\_numeric\_and\_nominal\_attributes.png}}
%\end{columns}
%\end{frame}
%\begin{frame}{Decision Trees and Random Forests}
%\begin{block}{Disadvantages of decision trees}
%\begin{itemize}
%\item comparatively inaccurate
%\item once you are in the wrong branch, you cannot go `back up'
%\item prone to overfitting (e.g., outlier in training data may lead to completely different outcome)
%\end{itemize}
%\end{block}
%\pause
%Therfore, nowadays people use \emph{random forests}: Random forests \emph{combine} the predictions of \emph{multiple} trees
%$\Rightarrow$ might be a good choice for your non-linear classification problem
%\end{frame}
%
%
%
%
%

\subsection{Classifiers}


\begin{frame}{Different classifiers}
Typical options in a nutshell:
\begin{itemize}
	\item Na\"ive Bayes
	\item Logistic Regression
	\item Support Vector Machine (SVM/SVC)
	\item Random forests
\end{itemize}
\end{frame}


\subsection{Vectorizers}

\begin{frame}{Different vectorizers}
\begin{enumerate}[<+->]
\item CountVectorizer (=simple word counts)
\item TfidfVectorizer (word counts (``term frequency'') weighted by number of documents in which the word occurs at all (``inverse document frequency''))
\end{enumerate}

\pause
$$tfidf_{t,d} = tf_{t,d} \cdot idf_{t}$$

\end{frame}








\begin{frame}{Which one would you (not) use for which purpose?}

\makebox[\linewidth]{
\includegraphics[width=\paperwidth,height=.8\paperheight,keepaspectratio]{imdbsml}}
\end{frame}




\section{Classification}



\begin{frame}{Let's consider three tasks}
	For a given text (say, a news article, a press release, a review), determine the
\begin{description}
	\item[sentiment] e.g., [positive|neutral|negative]
	\item[topic] e.g., [sports|economy|politics|entertainment|other]
	\item[frames] e.g., [economic|human|moral|conflict], or non-exclusive: economic = [0|1], human = [0|1], \ldots
\end{description}
\end{frame}



\question{Imagine using a dictionary-based (list of keywords, list of regular expressions, or similar) approach to these tasks. How does the design (length, inclusiveness, etc.) of this list influence precision and recall?}


\begin{frame}{Dictionary-based approaches for text classification}
\begin{columns}[t]
	\column{.5\textwidth}
	\begin{block}{good for}
\begin{itemize}
	\item distinct, manifest things (names of organizations, pronouns, swearwords (?), \ldots)
	\item little room for interpretation/misunderstandings etc.
	\item ``must-be-explainable-to-a-five-year-old''
\end{itemize}
\pause
	\end{block}
	\column{.5\textwidth}
	\begin{alertblock}{bad for}
		\begin{itemize}
			\item latent constructs and concepts
			\item implicit things
		\end{itemize}
			\pause
			Hence, \emph{not} state-of-the-art for
		\begin{itemize}
			\item topics
			\item frames
			\item sentiment
		\end{itemize}
\end{alertblock}
\end{columns}
\end{frame}


\begin{frame}{From dictionary approaches to SML}
\begin{itemize}[<+->]
	\item Early days of sentiment analysis: list of positive words, list of negative words, count what occurs most
	\item You can even \textit{buy} lists of words that are meant to measure constructs like ``positive emotions'' or even ``analytic'' or ``authentic'' language use from a psychologist (LIWC, \cite{Pennebaker2007})
\end{itemize}
\end{frame}

\question{What do you think? Can this even work}





\begin{frame}{Bag-of-words dictionary approaches to sentiment analysis}
	\begin{block}{con}
		\begin{itemize}
			\item simplistic assumptions
			\item e.g., intensifiers cannot be interpreted (``really'' in ``really good'' or ``really bad'')
			\item or, even more important, negations.
		\end{itemize}
	\end{block}
\end{frame}




\begin{frame}{Improving the BOW approach}
	\begin{block}{Example: Sentistrenght \parencite{Thelwall2012}}
		\begin{itemize}
			\item $-5\ldots-1$ and $+1\ldots+5$ instead of positive/negative
			\item spelling correction
			\item ``booster word list'' for strengthening/weakening the effect of the following word
			\item interpreting repeated letters (``baaaaaad''), CAPITALS and !!!
			\item idioms, negation 
		\end{itemize}
	\end{block}

VADER by \cite{Hutto2014} works in a similar way.
\pause
\textbf{Even though this is much less na\"ive than LIWC, for instance, the problem remains: Can we construct a dictionary that, \emph{irrespective of the context}, gives us a meaningful estimate of sentiment? }
\end{frame}





\begin{frame}[standout]
	Such an \textit{off-the-shelf} dictionary does not (and probably cannot) exist.
\end{frame}



%\begin{frame}{\cite{Boukes2020}: Sentiment analysis of economic news}
%	\makebox[\columnwidth]{
%		\includegraphics[width=\columnwidth,height=.8\paperheight,keepaspectratio]{boukes2019}}
%\end{frame}


\begin{frame}{\cite{Boukes2020}: Sentiment analysis of economic news}
	\makebox[\columnwidth]{
		\includegraphics[width=\columnwidth,height=.8\paperheight,keepaspectratio]{boukes2019b}}
\end{frame}


\begin{frame}{\cite{Boukes2020}: Sentiment analysis of economic news}
\begin{itemize}
	\item Dictionaries have low agreement with each other, and also with human coders
	\item Even their own dictionary didn't agree
	\item \textbf{This is not because these dictionaries are particularly bad!}. Main point: For such a complex and context-dependent task, a dictionary is just not the right tool.
\end{itemize}
\end{frame}




\begin{frame}{\cite{VanAtteveldt2021}: Extending \cite{Boukes2020} with SML}
``manual coding (using undergraduate students) yields the
best results 

[\ldots] A good second place is taken by crowd coding [\ldots]  


[\ldots] machine learning performs worse than both students' manual coding and crowd coding.
Reaching $\alpha = 0.50$ for deep learning (CNN) and slightly worse for classical machine learning (SVM; $\alpha = 0.41$, NB; $\alpha = 0.40$), machine learning still performs significantly better than chance. However, since these results are lower than generally accepted levels of inter-coder reliability [\ldots]

Finally, [\ldots] dictionaries [\ldots] perform worse than the machine
learning results and much worse than manual annotation [\ldots] [and] approximate chance agreement''\end{frame}






\begin{frame}{\cite{Vermeer2019}: Satisfaction with brands}
	\makebox[\columnwidth]{
		\includegraphics[width=\columnwidth,height=.8\paperheight,keepaspectratio]{../../pictures/vermeer2019}}
\end{frame}



\begin{frame}[standout]
SML is no panacea, but the most promising approach to analyzing large quantities of texts. Don't believe off-the-shelf packages that claim to do the work for you.
(For small datasets, just do it by hand.)
\end{frame}




\section{SML}

  




\begin{frame}{SML to code frames and topics}
	\begin{block}{Some work by \cite{Burscher2014} and \cite{Burscher2015} }
		\begin{itemize}
			\item Humans can code generic frames (human-interest, economic, \ldots)
			\item Humans can code topics from a pre-defined list 
			\item<2->\textbf{But it is very hard to formulate an explicit rule} \\(as in: code as 'Human Interest' if regular expression R is matched)
		\end{itemize}
		\onslide<3>$\Rightarrow$ This is where you need supervised machine learning!
	\end{block}


\end{frame}




{\setbeamercolor{background canvas}{bg=black}
	\begin{frame}[plain]
		\makebox[\linewidth]{
			\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{../../pictures/burscher2014}}
	\end{frame}
	
	\begin{frame}[plain]
		\makebox[\linewidth]{
			\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{../../pictures/burscher2015-a}}
	\end{frame}
	
	\begin{frame}[plain]
		\makebox[\linewidth]{
			\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{../../pictures/burscher2015-b}}
	\end{frame}
}





\begin{frame}{What does this mean for our research?}
	\begin{block}<2>{It we have 2,000 documents with manually coded frames and topics\ldots}
		\begin{itemize}
			\item we can use them to train a SML classifier
			\item which can code an unlimited number of new documents
			\item with an acceptable accuracy (at least for some of them)
		\end{itemize}
	\end{block}
	\onslide<2>{
		\tiny{Some easier tasks even need only 500 training documents, see \cite{Hopkins2010}}}.
\end{frame}



\subsection{An implementation}

\begin{frame}[fragile]{An implementation}
Let's say we have a list of tuples with movie reviews and their rating:
\begin{lstlisting}
reviews=[("This is a great movie",1),("Bad movie",-1), ... ...]
\end{lstlisting}
And a second list with an identical structure:
\begin{lstlisting}
test=[("Not that good",-1),("Nice film",1), ... ...]
\end{lstlisting}
Both are drawn from the same population, it is pure chance whether a specific review is on the one list or the other.\\
\tiny{Based on an example from \url{http://blog.dataquest.io/blog/naive-bayes-movies/}}
\end{frame}


\begin{frame}[fragile]{Training a A Naïve Bayes Classifier}
\begin{lstlisting}
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics

# This is just an efficient way of computing word counts
vectorizer = CountVectorizer(stop_words='english')
train_features = vectorizer.fit_transform([r[0] for r in reviews])
test_features = vectorizer.transform([r[0] for r in test])

# Fit a naive bayes model to the training data.
nb = MultinomialNB()
nb.fit(train_features, [r[1] for r in reviews])

# Now we can use the model to predict classifications for our test features.
predictions = nb.predict(test_features)
actual=[r[1] for r in test]

print("Precision: {0}".format(metrics.precision_score(actual, predictions, pos_label=1, labels = [-1,1])))
print("Recall: {0}".format(metrics.recall_score(actual, predictions, pos_label=1, labels = [-1,1])))
\end{lstlisting}
\end{frame}
%
%\begin{frame}{TODO}
%TODO\\
%andere vectorizer (TFIDF)\\
%verschillen classifiers\\
%andere output (metrics summary)
%\\
%waarom is dit hier een MULTINOMIAL NB
%\\
%scikit-learn installeren\\ 
%opdracht bedenken: classifiers vergelijken
%\end{frame}


\begin{frame}{And it works!}
Using 50,000 IMDB movies that are classified as either negative or positive,
\begin{itemize}
	\item I created a list with 25,000 training tuples and another one with 25,000 test tuples and
	\item trained a classifier
	\item with precision and recall values $>.80$
\end{itemize}
~\\
\tiny{Dataset obtained from \url{http://ai.stanford.edu/~amaas/data/sentiment}, Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., \& Potts, C. (2011). Learning word vectors for sentiment analysis. \emph{49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)}
}

\end{frame}

\begin{frame}[fragile]{Playing around with new data}
\begin{lstlisting}
newdata=vectorizer.transform(["What a crappy movie! It sucks!", "This is awsome. I liked this movie a lot, fantastic actors","I would not recomment it to anyone.", "Enjoyed it a lot"])
predictions = nb.predict(newdata)
print(predictions)
\end{lstlisting}
This returns, as you would expect and hope:
\begin{lstlisting} 
[-1  1 -1  1]
\end{lstlisting}


\end{frame}




\begin{frame}{But we can do even better}
We can use different vectorizers and different classifiers.
\end{frame}

\begin{frame}{ELI5}
\makebox[\linewidth]{
	\includegraphics[width=\paperwidth,height=.8\paperheight,keepaspectratio]{eli5}}
\end{frame}


%\section{Summing up}

%\subsection{Revisiting the difference between the dictionary approach and the SML}
%
%
%\begin{frame}{What \emph{is} our fitted classifier again?}
%Essentially, just a formula 
%
%	$$p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)}}$$
%
%where $\beta_0$ is an intercept\footnote{Machine Learning people sometimes call the intercept ``bias'' (yes, I know, that's confusing)}, $\beta_1$ a coefficient for the frequency (or tf$\cdot$ idf score) of some word, $\beta_2$ a coefficient some other word.
%
%If our fitted \emph{vectorizer} contains 5,000 words, we thus have 5,001 coefficients.
%
%\tiny{(for logistic regression in this case, but same argument applies to other classifiers as well)}
%
%\end{frame}
%
%\question{But isn't that then essentially very much like a dictionary, except that the words have different weights?}
%
%
%\begin{frame}{In some sense, yes.}
%\begin{itemize}
%	\item But we don't pretend that we can construct the dictionary \emph{a priori}.
%	\item It's specifically tailored to our use-case.
%	\item The weights are \emph{really} essential here.
%\end{itemize}
%
%\pause
%We \emph{could} print all coefficients-word pairs, but probably it's enough to just look at those with the largest absolute value:
%\end{frame}
%
%
%
%
%
%


\subsection{A note on the input data}

\begin{frame}{The input scikit-learn expects}
	A training dataset consisting of:
\begin{enumerate}
	\item an array (e.g., a list) of labels (\texttt{y\_train})
	\item a corresponding array (e.g., a list) of feature vectors (\texttt{X\_train})
\end{enumerate}

	A test dataset consisting of:
\begin{enumerate}
	\item an array (e.g., a list) of labels (\texttt{y\_test})
	\item a corresponding array (e.g., a list) of feature vectors (\texttt{X\_test})
\end{enumerate}

The feature vectors can be created via a \textit{vectorizer}, but could in principle also just be lists themselves.

We use a lowercase \texttt{y} because it is a onedimensional vector, and an uppercase \texttt{X} because it is a two-dimensional matrix.
\end{frame}


\begin{frame}{The input scikit-learn expects}
\begin{itemize}
	\item \textbf{It does not matter \emph{how} you create \texttt{y} and \texttt{X}!}
	\item Getting data into the right shape can be as much work (or more) as training the classifier itself
\end{itemize}
\pause
Typical techniques:
\begin{itemize}
	\item Reading text files from folders into lists of strings (looping over folder contents)
	\item Reading from csv file either directly into lists (\texttt{csv} module) or via pandas
	\item List comprehension to restructure or process data
	\item Potentially, you need to split into train and test dataset yourself (with slicing, or  with scikit-learn itself)
\end{itemize}


\end{frame}


\question{Any questions?}

\section{Looking back \& forward}




\begin{frame}{Steps of a CSS project}
We learned techniques for:
\begin{itemize}
	\item retrieving data
	\item processing data
	\item analyzing data
	\item visualising data
\end{itemize}

\end{frame}

\begin{frame}[plain]
\begin{tikzpicture}[node distance = 3cm, auto]
\node [cloud] (retrieve) {retrieve};
\node [cloud, right of=retrieve] (process) {process and/or enrich};
\node [cloud, right of=process] (analyze) {analyze\\ explain\\ predict};
\node [cloud, right of=analyze] (visualize) {communi-cate};


\path [pijltje] (retrieve)--(process);
\path [pijltje] (process)--(analyze);
\path [pijltje] (analyze)--(visualize);


\node [block, below of = retrieve] (retrievetech) {files\\ APIs\\ scraping};
\node [block, below of= process] (processtech) {NLP\\ sentiment\\ LDA\\ SML};
\node [block, below of=analyze] (analyzetech) {group comparisons; statistical tests and models};
\node [block, below of=visualize] (visualizetech) {visualizations and summary tables};



\path [pijltje] (retrievetech)--(processtech);
\path [pijltje] (processtech)--(analyzetech);
\path [pijltje] (analyzetech)--(visualizetech);


\node [block, below of = retrievetech, fill=green!20] (retrievepython) {glob\\ json \& csv\\ requests\\ twitter\\ tweepy\\ lxml\\ \ldots};

\node [block, below of = processtech, fill=green!20] (processpython) {nltk\\ gensim\\ scikit-learn \ldots};

\node [block, below of = analyzetech, fill=green!20] (analyzepython) {numpy/scipy\\ pandas\\ statsmodels\\ \ldots};

\node [block, below of = visualizetech, fill=green!20] (visualizepython) {pandas\\ matplotlib\\ seaborn\\ pyldavis\\ \ldots};





\path [line, dashed] (retrieve)--(retrievetech);
\path [line, dashed] (process)--(processtech);
\path [line, dashed] (analyze)--(analyzetech);
\path [line, dashed] (visualize)--(visualizetech);



\path [line, dashed] (retrievetech)--(retrievepython);
\path [line, dashed] (processtech)--(processpython);
\path [line, dashed] (analyzetech)--(analyzepython);
\path [line, dashed] (visualizetech)--(visualizepython);
\end{tikzpicture}
\end{frame}


\subsection{A good workflow}
\begin{frame}
A good workflow
\end{frame}




\begin{frame}{The big picture}
\begin{block}{Start with pen and paper}
\begin{enumerate}[<+->]
\item Draw the Big Picture
\item Then work out what components you need
\end{enumerate}
\end{block}
\end{frame}




\begin{frame}{Develop components separately}
\begin{block}{One script for downloading the data, one script for analyzing}
\begin{itemize}[<+->]
\item Avoids waste of resources (e.g., unnecessary downloading multiple times)
\item Makes it easier to re-use your code or apply it to other data
\end{itemize}
\end{block}
\pause
\begin{block}{Start small, then scale up}
\begin{itemize}[<+->]
\item Take your plan (see above) and solve \textit{one} problem at a time (e.g., parsing a review page; or getting the URLs of all review pages)
\item (for instance, by using functions [next slides])
\end{itemize}
\end{block}

\end{frame}	


\begin{frame}{Develop components separately}
\begin{block}{If you copy-paste code, you are doing something wrong}
\begin{itemize}[<+->]
\item Write loops!
\item If something takes more than a couple of lines, write a function!
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[plain, fragile]
Copy-paste approach\\ (ugly, error-prone, hard to scale up)
\begin{lstlisting}
allreviews = []

response = requests.get('http://xxxxx')
tree =  fromstring(response.text)
reviewelements = tree.xpath('//div[@class="review"]')
reviews = [e.text for e in reviewelements]
allreviews.extend(reviews)

response = requests.get('http://yyyyy')
tree =  fromstring(response.text)
reviewelements = tree.xpath('//div[@class="review"]')
reviews = [e.text for e in reviewelements]
allreviews.extend(reviews)
\end{lstlisting}
\end{frame}


\begin{frame}[plain, fragile]
Better: for-loop\\ (easier to read, less error-prone, easier to scale up (e.g., more URLs, read URLs from a file or existing list)))
\begin{lstlisting}
allreviews = []

urls = ['http://xxxxx', 'http://yyyyy']

for url in urls:
response = requests.get(url)
tree =  fromstring(response.text)
reviewelements = tree.xpath('//div[@class="review"]')
reviews = [e.text for e in reviewelements]
allreviews.extend(reviews)
\end{lstlisting}
\end{frame}




\begin{frame}[plain, fragile]
Even better: for-loop with functions\\ (main loop is easier to read, function can be re-used in multiple contexts)
\begin{lstlisting}
def getreviews(url):
response = requests.get(url)
tree =  fromstring(response.text)
reviewelements = tree.xpath('//div[@class="review"]')
return [e.text for e in reviewelements]


urls = ['http://xxxxx', 'http://yyyyy']

allreviews = []

for url in urls:
allreviews.extend(getreviews(url))
\end{lstlisting}
\end{frame}


\begin{frame}{Scaling up}
Until now, we did not look too much into aspects like code style, re-usability, scalability
\begin{itemize}
\item Use functions to make code more readable and re-usable
\item Avoid re-calculating values
\item Think about how to minimize memory usage (e.g., generators)
\item Do not hard-code values, file names, etc., but take them as arguments
\end{itemize}	
\end{frame}




\begin{frame}{Make it robust}
You cannot foresee every possible problem.\\
Most important: Make sure your program does not fail and loose all data just because something goes wrong at case 997/1000.
\begin{itemize}
\item Use \texttt{try/except} to explicitly tell the program how to handle errors
\item Write data to files (or database) in between
\item Use \texttt{assert len(x) == len(y}) for sanity checks
\end{itemize}	
\end{frame}



\begin{frame}{Practice yourself!}
\begin{itemize}
	\item Reproduce examples from the book for SML on the IMDB data (11.2, 11.3, 11.4) (check \texttt{week08/exercises/codefrombook.py} on github if you do not want to type over the code)
	\item Play around with different options! Can you tweak the models and make them even better? Take a look back at week 7 when we compared different vectorizers as well!
\end{itemize}
\end{frame}


\begin{frame}{Thursday meeting}
\begin{itemize}
	\item Sign up on canvas in case you have questions about your final project. 
	\item Deadline final project: Friday, 28-5, 23:59
\end{itemize}
\end{frame}

\begin{frame}[plain,allowframebreaks]
	\printbibliography
\end{frame}


\end{document}